{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d284fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from data_loader.datasets import load_data\n",
    "from evaluate import evaluate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "087bc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dropout, MaxPool1D, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, SpatialDropout1D\n",
    "from config import inputLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc919c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcnn1(tokenizer, class_num=2):\n",
    "    kernel_size = [1, 3, 5]\n",
    "    acti = 'relu'\n",
    "    my_input = Input(shape=(inputLen,), dtype='int32')\n",
    "    emb = Embedding(len(tokenizer.word_index) + 1, 20,\n",
    "                    input_length=inputLen)(my_input)\n",
    "    emb = SpatialDropout1D(0.2)(emb)\n",
    "\n",
    "    net = []\n",
    "    for kernel in kernel_size:\n",
    "        con = Conv1D(32, kernel, activation=acti, padding=\"same\", kernel_regularizer=l2(0.0005))(emb)\n",
    "        con = MaxPool1D(2)(con)\n",
    "        net.append(con)\n",
    "    net = concatenate(net, axis=-1)\n",
    "    # net = concatenate(net)\n",
    "    net = Flatten()(net)\n",
    "    net = Dropout(0.5)(net)\n",
    "    net = Dense(256, activation='relu')(net)\n",
    "    net = Dropout(0.5)(net)\n",
    "    net = Dense(class_num, activation='softmax', kernel_regularizer=l2(l=0.001))(net)\n",
    "    model = Model(inputs=my_input, outputs=net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02b1aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Train Job! \n"
     ]
    }
   ],
   "source": [
    "batch_size=20\n",
    "epoch_num=5\n",
    "model_option=1\n",
    "print(\"Start Train Job! \")\n",
    "start = time.time()\n",
    "textcnn1_model_dir='models'\n",
    "if model_option == 1:\n",
    "    tokenizer_file_path = os.path.join(textcnn1_model_dir, \"tokenizer.pickle\")\n",
    "\n",
    "elif model_option == 2:\n",
    "    tokenizer_file_path = os.path.join(textcnn2_model_dir, \"tokenizer.pickle\")\n",
    "\n",
    "elif model_option == 3:\n",
    "    tokenizer_file_path = os.path.join(textcnn3_model_dir, \"tokenizer.pickle\")\n",
    "\n",
    "else:\n",
    "    raise Exception(\"not supported model_option: {}\".format(model_option))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c2f74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_df:  (1294531, 2)\n",
      "bad_df:  (48138, 2)\n",
      "total:  (98138, 2)\n",
      "filter label: 1 or 0  (98138, 2)\n",
      "after drop_duplicates df:  (94636, 2)\n"
     ]
    }
   ],
   "source": [
    "train_datas, val_datas, test_datas, train_labels, val_labels, test_labels = load_data(validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d87bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "inputLen = 1024  # 256  # 512\n",
    "\n",
    "def train_tokenizer_with_val(train_datas, val_datas, test_datas, tokenizer_file_path):\n",
    "    tokenizer = Tokenizer(num_words=None,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
    "                          lower=True,\n",
    "                          split=\" \",\n",
    "                          char_level=False)\n",
    "    tokenizer.fit_on_texts(train_datas)\n",
    "    tokenizer.fit_on_texts(val_datas)\n",
    "    tokenizer.fit_on_texts(test_datas)\n",
    "    # print(tokenizer.word_index)\n",
    "    # # vocal = tokenizer.word_index\n",
    "    train_datas = tokenizer.texts_to_sequences(train_datas)\n",
    "    val_datas = tokenizer.texts_to_sequences(val_datas)\n",
    "    test_datas = tokenizer.texts_to_sequences(test_datas)\n",
    "    train_datas = pad_sequences(\n",
    "        train_datas, inputLen, padding='post', truncating='post')\n",
    "    val_datas = pad_sequences(\n",
    "        val_datas, inputLen, padding='post', truncating='post')\n",
    "    test_datas = pad_sequences(\n",
    "        test_datas, inputLen, padding='post', truncating='post')\n",
    "\n",
    "    with open(tokenizer_file_path, \"wb\") as tokenizer_file:\n",
    "        pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "    return tokenizer, train_datas, val_datas, test_datas\n",
    "\n",
    "tokenizer, train_datas, val_datas, test_datas = train_tokenizer_with_val(train_datas, val_datas, test_datas, tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d84123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1024, 20)     1227760     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 1024, 20)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1024, 32)     672         spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1024, 32)     1952        spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1024, 32)     3232        spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 512, 32)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 512, 32)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 512, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512, 96)      0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 49152)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 49152)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          12583168    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            514         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,817,298\n",
      "Trainable params: 13,817,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class_num = 2\n",
    "\n",
    "if model_option == 1:\n",
    "    model = textcnn1(tokenizer, class_num)\n",
    "    model_save_path = os.path.join(textcnn1_model_dir, 'model.h5')\n",
    "\n",
    "elif model_option == 2:\n",
    "    model = textcnn2(tokenizer, class_num)\n",
    "    model_save_path = os.path.join(textcnn2_model_dir, 'model.h5')\n",
    "\n",
    "elif model_option == 3:\n",
    "    model = textcnn3(tokenizer, class_num)\n",
    "    model_save_path = os.path.join(textcnn3_model_dir, 'model.h5')\n",
    "\n",
    "else:\n",
    "    raise Exception(\"not supported model_option: {}\".format(model_option))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97d820c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3786/3786 [==============================] - 856s 226ms/step - loss: 0.1326 - accuracy: 0.9681 - val_loss: 0.0999 - val_accuracy: 0.9793.1332 - \n",
      "Epoch 2/5\n",
      "3786/3786 [==============================] - 722s 191ms/step - loss: 0.0566 - accuracy: 0.9868 - val_loss: 0.0943 - val_accuracy: 0.9790\n",
      "Epoch 3/5\n",
      "3786/3786 [==============================] - 614s 162ms/step - loss: 0.0300 - accuracy: 0.9941 - val_loss: 0.1141 - val_accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "3786/3786 [==============================] - 618s 163ms/step - loss: 0.0215 - accuracy: 0.9960 - val_loss: 0.4092 - val_accuracy: 0.7665\n",
      "Epoch 5/5\n",
      "3786/3786 [==============================] - 675s 178ms/step - loss: 0.0178 - accuracy: 0.9968 - val_loss: 0.1053 - val_accuracy: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ed52089b48>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer = Adam(learning_rate=1e-3)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='min', baseline=None,restore_best_weights=True)\n",
    "model.fit(train_datas, train_labels, epochs=epoch_num, batch_size=batch_size,\n",
    "          validation_data=(val_datas, val_labels), callbacks=[checkpoint, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98372965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over train job in 3848.699727 s\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Over train job in %f s\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac8bb39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_true:  (9464, 2)\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(model_save_path)\n",
    "labels_true = test_labels\n",
    "print(\"labels_true: \", labels_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dbc18dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_pre:  (9464, 2)\n"
     ]
    }
   ],
   "source": [
    "labels_pre = model.predict(test_datas)\n",
    "print(\"labels_pre: \", labels_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18cd5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pre = np.array(labels_pre).round()\n",
    "\n",
    "def to_y(labels):\n",
    "    y = []\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "\n",
    "        if label[0] == 1:\n",
    "            y.append(0)\n",
    "\n",
    "        elif label[1] == 1:\n",
    "            y.append(1)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"not supported result: {}\".format(label))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75534804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is:  0.9792899408284024\n",
      "Precision Score is : 0.9828086626479124\n",
      "Recall Score is : 0.9736783897367839\n",
      "F1 Score:  0.9782222222222223\n",
      "AUC Score:  0.9790504026369534\n"
     ]
    }
   ],
   "source": [
    "y_true = to_y(labels_true)\n",
    "y_pre = to_y(labels_pre)\n",
    "\n",
    "evaluate_result(y_true, y_pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
